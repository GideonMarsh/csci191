{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following along with the lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# calculate GINI\n",
    "Gini = lambda Pl: sum([p * (1-p) for p in Pl])\n",
    "\n",
    "# calculate Entropy\n",
    "Entropy = lambda Pl: -1 * sum([p * math.log(p, 2) if p else 0 for p in Pl]) \n",
    "\n",
    "# compute probabilities\n",
    "Probs = lambda Lx: [i/sum(Lx) for i in Lx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on A1\n",
      "Entropy left side:  -0.0\n",
      "Entropy right side:  1.0\n",
      "Total entropy after split:  0.8\n",
      "Information gain:  0.17095059445466854 \n",
      "\n",
      "Split on A2\n",
      "Entropy left side:  -0.0\n",
      "Entropy right side:  0.9182958340544896\n",
      "Total entropy after split:  0.5509775004326937\n",
      "Information gain:  0.4199730940219749 \n",
      "\n",
      "Split on A3\n",
      "Entropy left side:  0.9182958340544896\n",
      "Entropy right side:  1.0\n",
      "Total entropy after split:  0.9509775004326937\n",
      "Information gain:  0.01997309402197489 \n",
      "\n",
      "A2 has most information gain, so split on that for first branch\n"
     ]
    }
   ],
   "source": [
    "X = [[1,0,0],[1,0,1],[0,1,0],[1,1,1],[1,0,1]]\n",
    "Y = [0,0,0,1,1]\n",
    "\n",
    "# First branch of tree (root)\n",
    "\n",
    "Binary = [3,2]\n",
    "\n",
    "BinarySplit1 = [[1,0],[2,2]]\n",
    "BinarySplit2 = [[2,0],[1,2]]\n",
    "BinarySplit3 = [[2,1],[1,1]]\n",
    "\n",
    "Before = Entropy(Probs(Binary))\n",
    "\n",
    "print(\"Split on A1\")\n",
    "print(\"Entropy left side: \", Entropy(Probs([1,0])))\n",
    "print(\"Entropy right side: \", Entropy(Probs([2,2])))\n",
    "After = 1.0/5.0 * Entropy(Probs([1,0])) + 4.0/5.0 * Entropy(Probs([2,2]))\n",
    "print(\"Total entropy after split: \", After)\n",
    "print(\"Information gain: \", Before - After, '\\n')\n",
    "\n",
    "print(\"Split on A2\")\n",
    "print(\"Entropy left side: \", Entropy(Probs([2,0])))\n",
    "print(\"Entropy right side: \", Entropy(Probs([1,2])))\n",
    "After = 2.0/5.0 * Entropy(Probs([2,0])) + 3.0/5.0 * Entropy(Probs([1,2]))\n",
    "print(\"Total entropy after split: \", After)\n",
    "print(\"Information gain: \", Before - After, '\\n')\n",
    "\n",
    "print(\"Split on A3\")\n",
    "print(\"Entropy left side: \", Entropy(Probs([2,1])))\n",
    "print(\"Entropy right side: \", Entropy(Probs([1,1])))\n",
    "After = 3.0/5.0 * Entropy(Probs([2,1])) + 2.0/5.0 * Entropy(Probs([1,1]))\n",
    "print(\"Total entropy after split: \", After)\n",
    "print(\"Information gain: \", Before - After, '\\n')\n",
    "print(\"A2 has most information gain, so split on that for first branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy left side:  -0.0\n",
      "Left side pure, no further splits needed \n",
      "\n",
      "Entropy right side:  0.9182958340544896\n",
      "Split on A1\n",
      "Entropy left side:  -0.0\n",
      "Entropy right side:  -0.0\n",
      "Total entropy after split:  -0.0\n",
      "Information gain:  0.9182958340544896 \n",
      "\n",
      "No split on A2, since that was previous split \n",
      "\n",
      "Split on A3\n",
      "Entropy left side:  1.0\n",
      "Entropy right side:  -0.0\n",
      "Total entropy after split:  0.6666666666666666\n",
      "Information gain:  0.2516291673878229 \n",
      "\n",
      "A1 has most information gain, so split on that\n",
      "Now completely pure, decision tree complete\n"
     ]
    }
   ],
   "source": [
    "# second branch of tree (after A2 split)\n",
    "\n",
    "print(\"Entropy left side: \", Entropy(Probs([2,0])))\n",
    "print(\"Left side pure, no further splits needed\", '\\n')\n",
    "\n",
    "print(\"Entropy right side: \", Entropy(Probs([1,2])))\n",
    "Before = Entropy(Probs([1,2]))\n",
    "\n",
    "print(\"Split on A1\")\n",
    "print(\"Entropy left side: \", Entropy(Probs([1,0])))\n",
    "print(\"Entropy right side: \", Entropy(Probs([0,2])))\n",
    "After = 1.0/3.0 * Entropy(Probs([1,0])) + 2.0/3.0 * Entropy(Probs([0,2]))\n",
    "print(\"Total entropy after split: \", After)\n",
    "print(\"Information gain: \", Before - After, '\\n')\n",
    "\n",
    "print(\"No split on A2, since that was previous split\", '\\n')\n",
    "\n",
    "print(\"Split on A3\")\n",
    "print(\"Entropy left side: \", Entropy(Probs([1,1])))\n",
    "print(\"Entropy right side: \", Entropy(Probs([0,1])))\n",
    "After = 2.0/3.0 * Entropy(Probs([1,1])) + 1.0/3.0 * Entropy(Probs([0,1]))\n",
    "print(\"Total entropy after split: \", After)\n",
    "print(\"Information gain: \", Before - After, '\\n')\n",
    "print(\"A1 has most information gain, so split on that\")\n",
    "print(\"Now completely pure, decision tree complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
